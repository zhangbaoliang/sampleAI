{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "217f4c6c-6862-429c-b6d6-a320803c56d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "import hashlib\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "import langchain\n",
    "load_dotenv()\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "#langchain.debug = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36a4d8fa-9aef-47de-9e4e-3bf04dc9425f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#persist_directory = './db/chroma/api_doc'\n",
    "#vectordb = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c42934f5-0eeb-4c2c-bf74-da218503e71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.chat_models import ChatOpenAI, ChatVertexAI\n",
    "from langchain.chat_models.anthropic import ChatAnthropic\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "import LANGUAGE_PROMPT\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\", return_messages=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d117f185-36cb-4c03-8039-00b594f58312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"aiRecognize使用示例代码\",\n",
      "  \"chat_history\": []\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 2:chain:StuffDocumentsChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input_documents\": [],\n",
      "  \"question\": \"aiRecognize使用示例代码\",\n",
      "  \"chat_history\": \"\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 2:chain:StuffDocumentsChain > 3:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"aiRecognize使用示例代码\",\n",
      "  \"context\": \"\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 2:chain:StuffDocumentsChain > 3:chain:LLMChain > 4:llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: Use the following pieces of context to answer the users question. \\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n----------------\\n\\nHuman: aiRecognize使用示例代码\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 2:chain:StuffDocumentsChain > 3:chain:LLMChain > 4:llm:ChatOpenAI] [3.17s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"我不确定您指的是哪个aiRecognize，因为这个词可能指代多个不同的软件或服务。请提供更多上下文或信息，以便我能够更好地回答您的问题。\",\n",
      "        \"generation_info\": null,\n",
      "        \"message\": {\n",
      "          \"content\": \"我不确定您指的是哪个aiRecognize，因为这个词可能指代多个不同的软件或服务。请提供更多上下文或信息，以便我能够更好地回答您的问题。\",\n",
      "          \"additional_kwargs\": {},\n",
      "          \"example\": false\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"prompt_tokens\": 60,\n",
      "      \"completion_tokens\": 61,\n",
      "      \"total_tokens\": 121\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 2:chain:StuffDocumentsChain > 3:chain:LLMChain] [3.17s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"我不确定您指的是哪个aiRecognize，因为这个词可能指代多个不同的软件或服务。请提供更多上下文或信息，以便我能够更好地回答您的问题。\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 2:chain:StuffDocumentsChain] [3.17s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output_text\": \"我不确定您指的是哪个aiRecognize，因为这个词可能指代多个不同的软件或服务。请提供更多上下文或信息，以便我能够更好地回答您的问题。\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain] [3.82s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"answer\": \"我不确定您指的是哪个aiRecognize，因为这个词可能指代多个不同的软件或服务。请提供更多上下文或信息，以便我能够更好地回答您的问题。\"\n",
      "}\n",
      "我不确定您指的是哪个aiRecognize，因为这个词可能指代多个不同的软件或服务。请提供更多上下文或信息，以便我能够更好地回答您的问题。\n"
     ]
    }
   ],
   "source": [
    "\n",
    "langchain.debug = True\n",
    "persist_directory = './db/chroma/api_doc'\n",
    "\n",
    "db = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n",
    "\n",
    "#retriever = db.as_retriever(search_type=\"mmr\")\n",
    "#retriever = db.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": .3})\n",
    "retriever = db.as_retriever()\n",
    "examples = ['aiRecognize有哪些参数?', 'aiRecognize需要哪些权限', '请提供aiRecognize的示例代码?']\n",
    "examples = ['aiRecognize有哪些参数']\n",
    "examples = ['我想实现视觉相关功能有哪些API可以支持']\n",
    "\n",
    "examples = ['aiRecognizeAPI使用相关的示例代码']\n",
    "examples = [\"aiRecognize使用示例代码\"]\n",
    "\n",
    "openai_api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "ConversationalRetrievalChain.prompts = LANGUAGE_PROMPT\n",
    "qa = None\n",
    "# this overwrites the built-in prompt of the ConversationalRetrievalChain\n",
    "ConversationalRetrievalChain.prompts = LANGUAGE_PROMPT\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    ChatOpenAI(\n",
    "        model_name=\"gpt-3.5-turbo\", openai_api_key=openai_api_key, \n",
    "        temperature=\"0.0\", max_tokens=1024), \n",
    "        db.as_retriever(), memory=memory, verbose=True, max_tokens_limit=1024)\n",
    "for doc in examples:    \n",
    "    query = doc\n",
    "    model_response = qa({\"question\": doc})    \n",
    "    print(model_response['answer'])\n",
    "    #time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5ee1d595-fb0c-40ee-93db-951f71c3bfc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:VectorDBQA] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"query\": \"aiRecognize有哪些参数\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:VectorDBQA > 2:chain:StuffDocumentsChain] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:VectorDBQA > 2:chain:StuffDocumentsChain > 3:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"aiRecognize有哪些参数\",\n",
      "  \"context\": \"kl.aiRecognize\\n\\nAPI名称\\n\\naiRecognize\\n\\n功能描述\\n\\n快驴容器增加视觉识别能力\\n\\n参数说明\\n\\n输入参数\\n\\nschema: 'aiRecognize',\\n})\\n// 调用异步 api\\nif (isSupport) {\\n  myapi.callAsync({\\n    appId: 'value',\\n    businessId: 'value',\\n    isOnline: 'value',\\n    cardType: 'value',\\n    inputCardMode: 'value',\\n\\nretryCount: 'value',\\n    maxScanTime: 'value',\\n    flashMode: 'value',\\n    capacity: 'value',\\n    target: 'value',\\n    strTag: 'value',\\n    maxHeight: 'value',\\n    maxWidth: 'value',\\n    imgPath: 'value',\\n\\n// 创建 api\\nconst myapi = wx.registerExtendedAPI({\\n  supportAsync: True,\\n  supportSync: False,\\n  name: 'aiRecognize',\\n  scope: 'kl',\\n})\\n// 判断当前 App api 是否可用\\nconst isSupport = wx.canIUse({\\n  scope: 'kl',\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:VectorDBQA > 2:chain:StuffDocumentsChain > 3:chain:LLMChain > 4:llm:OpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\nkl.aiRecognize\\n\\nAPI名称\\n\\naiRecognize\\n\\n功能描述\\n\\n快驴容器增加视觉识别能力\\n\\n参数说明\\n\\n输入参数\\n\\nschema: 'aiRecognize',\\n})\\n// 调用异步 api\\nif (isSupport) {\\n  myapi.callAsync({\\n    appId: 'value',\\n    businessId: 'value',\\n    isOnline: 'value',\\n    cardType: 'value',\\n    inputCardMode: 'value',\\n\\nretryCount: 'value',\\n    maxScanTime: 'value',\\n    flashMode: 'value',\\n    capacity: 'value',\\n    target: 'value',\\n    strTag: 'value',\\n    maxHeight: 'value',\\n    maxWidth: 'value',\\n    imgPath: 'value',\\n\\n// 创建 api\\nconst myapi = wx.registerExtendedAPI({\\n  supportAsync: True,\\n  supportSync: False,\\n  name: 'aiRecognize',\\n  scope: 'kl',\\n})\\n// 判断当前 App api 是否可用\\nconst isSupport = wx.canIUse({\\n  scope: 'kl',\\n\\nQuestion: aiRecognize有哪些参数\\nHelpful Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:VectorDBQA > 2:chain:StuffDocumentsChain > 3:chain:LLMChain > 4:llm:OpenAI] [1.76s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \" inputCardMode, retryCount, maxScanTime, flashMode, capacity, target, strTag, maxHeight, maxWidth, imgPath.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 31,\n",
      "      \"prompt_tokens\": 399,\n",
      "      \"total_tokens\": 430\n",
      "    },\n",
      "    \"model_name\": \"text-davinci-003\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:VectorDBQA > 2:chain:StuffDocumentsChain > 3:chain:LLMChain] [1.76s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \" inputCardMode, retryCount, maxScanTime, flashMode, capacity, target, strTag, maxHeight, maxWidth, imgPath.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:VectorDBQA > 2:chain:StuffDocumentsChain] [1.76s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output_text\": \" inputCardMode, retryCount, maxScanTime, flashMode, capacity, target, strTag, maxHeight, maxWidth, imgPath.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:VectorDBQA] [2.38s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"result\": \" inputCardMode, retryCount, maxScanTime, flashMode, capacity, target, strTag, maxHeight, maxWidth, imgPath.\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' inputCardMode, retryCount, maxScanTime, flashMode, capacity, target, strTag, maxHeight, maxWidth, imgPath.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import VectorDBQA\n",
    "\n",
    "qa = VectorDBQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", vectorstore=db)\n",
    "query = \"aiRecognize有哪些参数\"\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fe8784-4508-4699-8b6b-fdfd5ec5240c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0c0d390c-64e5-48c7-ac2b-07e6cb53f798",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFileContext(filepath):\n",
    "    # 用'with'关键字自动关闭文件\n",
    "    with open('filename.txt', 'r') as f:\n",
    "        content = f.read()\n",
    "    #print(content)\n",
    "    return content\n",
    "\n",
    "def convert_bytes(bytes, precision=2):\n",
    "    \"\"\"Converts bytes into a human-friendly format.\"\"\"\n",
    "    abbreviations = ['B', 'KB', 'MB']\n",
    "    if bytes <= 0:\n",
    "        return '0 B'\n",
    "    size = bytes\n",
    "    index = 0\n",
    "    while size >= 1024 and index < len(abbreviations) - 1:\n",
    "        size /= 1024\n",
    "        index += 1\n",
    "    return f'{size:.{precision}f} {abbreviations[index]}'\n",
    "\n",
    "def get_file_size(filepath):\n",
    "    file_size = os.path.getsize(filepath)\n",
    "    return file_size\n",
    "\n",
    "def compute_sha1_from_file(file_path):\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        bytes = file.read()\n",
    "        readable_hash = compute_sha1_from_content(bytes)\n",
    "    return readable_hash\n",
    "\n",
    "\n",
    "def compute_sha1_from_content(content):\n",
    "    readable_hash = hashlib.sha1(content).hexdigest()\n",
    "    return readable_hash    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "33ad4d28-8237-425d-b9d7-2c5d3eb24e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadFile(filepath):\n",
    "    loader = UnstructuredMarkdownLoader(filepath)\n",
    "    documents = loader.load()\n",
    "    file_sha1 = compute_sha1_from_file(filepath)\n",
    "    file_size = get_file_size(filepath)\n",
    "    return documents, file_sha1, file_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c66b09d6-17c8-4a7b-968f-59db45239d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './api_docs/aiRecognize.md'\n",
    "chunk_size = 500\n",
    "chunk_overlap = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7663e169-4c7e-4ff6-b065-ff73e2931f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation.qa import QAGenerateChain\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "05afa507-66f3-40e4-9617-45987579f776",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "file_documents, file_sha1, file_size = loadFile(file_path)\n",
    "example_gen_chain = QAGenerateChain.from_llm(ChatOpenAI())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3e982f7c-bf93-49d7-b9fd-16d12eec9349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:QAGenerateChain] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:QAGenerateChain > 2:llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a teacher coming up with questions to ask on a quiz. \\nGiven the following document, please generate a question and answer based on that document.\\n\\nExample Format:\\n<Begin Document>\\n...\\n<End Document>\\nQUESTION: question here\\nANSWER: answer here\\n\\nThese questions should be detailed and be based explicitly on information in the document. Begin!\\n\\n<Begin Document>\\npage_content=\\\"kl.aiRecognize\\\\n\\\\nAPI名称\\\\n\\\\naiRecognize\\\\n\\\\n功能描述\\\\n\\\\n快驴容器增加视觉识别能力\\\\n\\\\n参数说明\\\\n\\\\n输入参数\\\\n\\\\n| 属性 | 类型 | 默认值 | 必填 | 说明 | 最大值 | 最小值 | 枚举值 |\\\\n| --- | --- | --- | --- | --- | --- | --- | --- |\\\\n| appId | int |  -  | True | appId | - | - | - |\\\\n| businessId | string |  -  | True | businessId | - | - | - |\\\\n| isOnline | boolean |  -  | False | isOnline | - | - | - |\\\\n| cardType | int |  -  | True | cardType | - | - | - |\\\\n| inputCardMode | int |  -  | True | inputCardMode | - | - | - |\\\\n| retryCount | int |  -  | False | retryCount | - | - | - |\\\\n| maxScanTime | int |  -  | False | maxScanTime | - | - | - |\\\\n| flashMode | int |  -  | False | flashMode | - | - | - |\\\\n| capacity | int |  -  | False | capacity | - | - | - |\\\\n| target | string |  -  | False | target | - | - | - |\\\\n| strTag | string |  -  | True | strTag | - | - | - |\\\\n| maxHeight | int |  -  | True | maxHeight | - | - | - |\\\\n| maxWidth | int |  -  | False | maxWidth | - | - | - |\\\\n| imgPath | string |  -  | False | imgPath | - | - | - |\\\\n| strReserve | string |  -  | False | strReserve | - | - | - |\\\\n| _mt | object |  -  | True | _mt | - | - | - |\\\\n| 结构属性 | 类型 | 说明 |\\\\n| sceneToken | string | 隐私token |\\\\n\\\\n返回参数\\\\n\\\\n| 属性 | 类型 | 说明 |\\\\n| --- | --- | --- |\\\\n| imgPath | string | imgPath |\\\\n| recResult | string | recResult |\\\\n| type | int | type |\\\\n| code | int | code |\\\\n| message | string | message |\\\\n\\\\n示例代码\\\\n\\\\njavascript\\\\n// 仅供业务同学参考，具体代码需要基于业务实际场景实现\\\\nif (typeof mmp === 'undefined' || typeof mmp.registerExtendedEvent !== 'function') {\\\\n  return\\\\n}\\\\n// 创建 api\\\\nconst myapi = wx.registerExtendedAPI({\\\\n  supportAsync: True,\\\\n  supportSync: False,\\\\n  name: 'aiRecognize',\\\\n  scope: 'kl',\\\\n})\\\\n// 判断当前 App api 是否可用\\\\nconst isSupport = wx.canIUse({\\\\n  scope: 'kl',\\\\n  schema: 'aiRecognize',\\\\n})\\\\n// 调用异步 api\\\\nif (isSupport) {\\\\n  myapi.callAsync({\\\\n    appId: 'value',\\\\n    businessId: 'value',\\\\n    isOnline: 'value',\\\\n    cardType: 'value',\\\\n    inputCardMode: 'value',\\\\n    retryCount: 'value',\\\\n    maxScanTime: 'value',\\\\n    flashMode: 'value',\\\\n    capacity: 'value',\\\\n    target: 'value',\\\\n    strTag: 'value',\\\\n    maxHeight: 'value',\\\\n    maxWidth: 'value',\\\\n    imgPath: 'value',\\\\n    strReserve: 'value',\\\\n    _mt: 'value',\\\\n    success(res) {\\\\n      console.log('success', res)\\\\n    },\\\\n    fail(err) {\\\\n      console.log('fail', err)\\\\n    }\\\\n  })\\\\n} else {\\\\n  // 此处建议业务填写兜底、兼容、上报等逻辑的代码\\\\n}\\\\n\\\\n隐私信息\\\\n\\\\nAndroid需要隐私权限: PermissionGuard.PERMISSION_CAMERA,PermissionGuard.PERMISSION_STORAGE_READ\\\\niOS需要隐私权限: PMPermissionIDCamera\\\" metadata={'source': './api_docs/aiRecognize.md'}\\n<End Document>\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:QAGenerateChain > 2:llm:ChatOpenAI] [4.00s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"QUESTION: What is the purpose of the aiRecognize API?\\n\\nANSWER: The aiRecognize API is intended to add visual recognition capability to the Quick Donkey container.\",\n",
      "        \"generation_info\": null,\n",
      "        \"message\": {\n",
      "          \"content\": \"QUESTION: What is the purpose of the aiRecognize API?\\n\\nANSWER: The aiRecognize API is intended to add visual recognition capability to the Quick Donkey container.\",\n",
      "          \"additional_kwargs\": {},\n",
      "          \"example\": false\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"prompt_tokens\": 1073,\n",
      "      \"completion_tokens\": 35,\n",
      "      \"total_tokens\": 1108\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:QAGenerateChain] [4.01s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"outputs\": [\n",
      "    {\n",
      "      \"text\": \"QUESTION: What is the purpose of the aiRecognize API?\\n\\nANSWER: The aiRecognize API is intended to add visual recognition capability to the Quick Donkey container.\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Could not parse output: QUESTION: What is the purpose of the aiRecognize API?\n\nANSWER: The aiRecognize API is intended to add visual recognition capability to the Quick Donkey container.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m vectordb \u001b[38;5;241m=\u001b[39m Chroma(persist_directory\u001b[38;5;241m=\u001b[39mpersist_directory, embedding_function\u001b[38;5;241m=\u001b[39membeddings)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#documents = text_splitter.split_documents(file_documents)\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m new_examples \u001b[38;5;241m=\u001b[39m \u001b[43mexample_gen_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_and_parse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdoc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfile_documents\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(new_examples)\n",
      "File \u001b[0;32m~/miniconda3/envs/pyjupyter/lib/python3.10/site-packages/langchain/chains/llm.py:257\u001b[0m, in \u001b[0;36mLLMChain.apply_and_parse\u001b[0;34m(self, input_list, callbacks)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Call apply and then parse the results.\"\"\"\u001b[39;00m\n\u001b[1;32m    256\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply(input_list, callbacks\u001b[38;5;241m=\u001b[39mcallbacks)\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pyjupyter/lib/python3.10/site-packages/langchain/chains/llm.py:263\u001b[0m, in \u001b[0;36mLLMChain._parse_result\u001b[0;34m(self, result)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_parse_result\u001b[39m(\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28mself\u001b[39m, result: List[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]]\n\u001b[1;32m    261\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Sequence[Union[\u001b[38;5;28mstr\u001b[39m, List[\u001b[38;5;28mstr\u001b[39m], Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompt\u001b[38;5;241m.\u001b[39moutput_parser \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 263\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    264\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompt\u001b[38;5;241m.\u001b[39moutput_parser\u001b[38;5;241m.\u001b[39mparse(res[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key]) \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m result\n\u001b[1;32m    265\u001b[0m         ]\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    267\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/envs/pyjupyter/lib/python3.10/site-packages/langchain/chains/llm.py:264\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_parse_result\u001b[39m(\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28mself\u001b[39m, result: List[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]]\n\u001b[1;32m    261\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Sequence[Union[\u001b[38;5;28mstr\u001b[39m, List[\u001b[38;5;28mstr\u001b[39m], Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompt\u001b[38;5;241m.\u001b[39moutput_parser \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    263\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m--> 264\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_parser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_key\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m result\n\u001b[1;32m    265\u001b[0m         ]\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    267\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/envs/pyjupyter/lib/python3.10/site-packages/langchain/output_parsers/regex.py:28\u001b[0m, in \u001b[0;36mRegexParser.parse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_output_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 28\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not parse output: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     31\u001b[0m             key: text \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_output_key \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     32\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_keys\n\u001b[1;32m     33\u001b[0m         }\n",
      "\u001b[0;31mValueError\u001b[0m: Could not parse output: QUESTION: What is the purpose of the aiRecognize API?\n\nANSWER: The aiRecognize API is intended to add visual recognition capability to the Quick Donkey container."
     ]
    }
   ],
   "source": [
    "#print(len(file_documents))\n",
    "langchain.debug = True\n",
    "persist_directory = './db/chroma_800/api_doc'\n",
    "vectordb = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n",
    "#documents = text_splitter.split_documents(file_documents)\n",
    "new_examples = example_gen_chain.apply_and_parse(\n",
    "    [{\"doc\": t} for t in file_documents]\n",
    ")\n",
    "print(new_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c8f0ab66-dee4-41ac-ae0e-584749add70c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RetrievalQA] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"query\": \"What is the purpose of the aiRecognize API?\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 20 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 2:chain:StuffDocumentsChain] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 2:chain:StuffDocumentsChain > 3:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What is the purpose of the aiRecognize API?\",\n",
      "  \"context\": \"kl.aiRecognize\\n\\nAPI名称\\n\\naiRecognize\\n\\n功能描述\\n\\n快驴容器增加视觉识别能力\\n\\n参数说明\\n\\n输入参数\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 2:chain:StuffDocumentsChain > 3:chain:LLMChain > 4:llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: Use the following pieces of context to answer the users question. \\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n----------------\\nkl.aiRecognize\\n\\nAPI名称\\n\\naiRecognize\\n\\n功能描述\\n\\n快驴容器增加视觉识别能力\\n\\n参数说明\\n\\n输入参数\\nHuman: What is the purpose of the aiRecognize API?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 2:chain:StuffDocumentsChain > 3:chain:LLMChain > 4:llm:ChatOpenAI] [1.10s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"The aiRecognize API is used to add visual recognition capabilities to the Fast Donkey container.\",\n",
      "        \"generation_info\": null,\n",
      "        \"message\": {\n",
      "          \"content\": \"The aiRecognize API is used to add visual recognition capabilities to the Fast Donkey container.\",\n",
      "          \"additional_kwargs\": {},\n",
      "          \"example\": false\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"prompt_tokens\": 101,\n",
      "      \"completion_tokens\": 19,\n",
      "      \"total_tokens\": 120\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 2:chain:StuffDocumentsChain > 3:chain:LLMChain] [1.10s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"The aiRecognize API is used to add visual recognition capabilities to the Fast Donkey container.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 2:chain:StuffDocumentsChain] [1.11s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output_text\": \"The aiRecognize API is used to add visual recognition capabilities to the Fast Donkey container.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RetrievalQA] [1.85s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"result\": \"The aiRecognize API is used to add visual recognition capabilities to the Fast Donkey container.\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The aiRecognize API is used to add visual recognition capabilities to the Fast Donkey container.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatOpenAI(temperature = 0.0)\n",
    "retriever = vectordb.as_retriever(search_type=\"mmr\")\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever, \n",
    "    verbose=True,\n",
    "    chain_type_kwargs = {\n",
    "        \"document_separator\": \"<<<<>>>>>\"\n",
    "    }\n",
    ")\n",
    "qa.run(new_examples[0][\"query\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bcbf1e94-5d0f-4f51-8c33-75ad38e879c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain.debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "360fc84c-6fff-4006-9722-346f8beed920",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'qa' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mqa\u001b[49m\u001b[38;5;241m.\u001b[39mapply(new_examples)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mqa\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m QAEvalChain\n",
      "\u001b[0;31mNameError\u001b[0m: name 'qa' is not defined"
     ]
    }
   ],
   "source": [
    "predictions = qa.apply(new_examples)\n",
    "from langchain.evaluation.qa import QAEvalChain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "236b19c9-3394-4adf-96f3-1689499e2c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0)\n",
    "eval_chain = QAEvalChain.from_llm(llm)\n",
    "graded_outputs = eval_chain.evaluate(new_examples, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2e99f2e2-efa2-4fa3-bd76-223f572633a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 0:\n",
      "Question: What is the purpose of the aiRecognize API?\n",
      "Real Answer: The aiRecognize API adds visual recognition capabilities to the Quick Donkey container.\n",
      "Predicted Answer: The aiRecognize API is used to add visual recognition capabilities to the Fast Donkey container.\n",
      "Predicted Grade: CORRECT\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, eg in enumerate(new_examples):\n",
    "    print(f\"Example {i}:\")\n",
    "    print(\"Question: \" + predictions[i]['query'])\n",
    "    print(\"Real Answer: \" + predictions[i]['answer'])\n",
    "    print(\"Predicted Answer: \" + predictions[i]['result'])\n",
    "    print(\"Predicted Grade: \" + graded_outputs[i]['text'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d58722-8877-473e-8e5b-fb13eb42f21d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c68075-460b-4300-928c-2c87cbcdb9e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c886a7-7d8e-41de-86c8-f8d37a92cd1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "217f4c6c-6862-429c-b6d6-a320803c56d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "OPENAI_API_KEY = \"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\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "import hashlib\n",
    "from langchain.vectorstores import Chroma\n",
    "embeddings = OpenAIEmbeddings()\n",
    "import langchain\n",
    "langchain.debug = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36a4d8fa-9aef-47de-9e4e-3bf04dc9425f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#persist_directory = './db/chroma/api_doc'\n",
    "#vectordb = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c42934f5-0eeb-4c2c-bf74-da218503e71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.chat_models import ChatOpenAI, ChatVertexAI\n",
    "from langchain.chat_models.anthropic import ChatAnthropic\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d117f185-36cb-4c03-8039-00b594f58312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"aiRecognize有哪些参数?\",\n",
      "  \"chat_history\": []\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 2:chain:StuffDocumentsChain] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 2:chain:StuffDocumentsChain > 3:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"aiRecognize有哪些参数?\",\n",
      "  \"context\": \"kl.aiRecognize\\n\\nAPI名称\\n\\naiRecognize\\n\\n功能描述\\n\\n快驴容器增加视觉识别能力\\n\\n参数说明\\n\\n输入参数\\n\\n隐私信息\\n\\nAndroid需要隐私权限: PermissionGuard.PERMISSION_CAMERA,PermissionGuard.PERMISSION_STORAGE_READ\\niOS需要隐私权限: PMPermissionIDCamera\\n\\n| inputCardMode | int |  -  | True | inputCardMode | - | - | - |\\n| retryCount | int |  -  | False | retryCount | - | - | - |\\n| maxScanTime | int |  -  | False | maxScanTime | - | - | - |\\n| flashMode | int |  -  | False | flashMode | - | - | - |\\n\\nstrReserve: 'value',\\n    _mt: 'value',\\n    success(res) {\\n      console.log('success', res)\\n    },\\n    fail(err) {\\n      console.log('fail', err)\\n    }\\n  })\\n} else {\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 2:chain:StuffDocumentsChain > 3:chain:LLMChain > 4:llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: Use the following pieces of context to answer the users question. \\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n----------------\\nkl.aiRecognize\\n\\nAPI名称\\n\\naiRecognize\\n\\n功能描述\\n\\n快驴容器增加视觉识别能力\\n\\n参数说明\\n\\n输入参数\\n\\n隐私信息\\n\\nAndroid需要隐私权限: PermissionGuard.PERMISSION_CAMERA,PermissionGuard.PERMISSION_STORAGE_READ\\niOS需要隐私权限: PMPermissionIDCamera\\n\\n| inputCardMode | int |  -  | True | inputCardMode | - | - | - |\\n| retryCount | int |  -  | False | retryCount | - | - | - |\\n| maxScanTime | int |  -  | False | maxScanTime | - | - | - |\\n| flashMode | int |  -  | False | flashMode | - | - | - |\\n\\nstrReserve: 'value',\\n    _mt: 'value',\\n    success(res) {\\n      console.log('success', res)\\n    },\\n    fail(err) {\\n      console.log('fail', err)\\n    }\\n  })\\n} else {\\nHuman: aiRecognize有哪些参数?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 2:chain:StuffDocumentsChain > 3:chain:LLMChain > 4:llm:ChatOpenAI] [18.09s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"aiRecognize有以下参数：\\n\\n- inputCardMode：int类型，必填项，表示输入卡片的模式。\\n- retryCount：int类型，非必填项，表示识别失败后的重试次数。\\n- maxScanTime：int类型，非必填项，表示最长的扫描时间。\\n- flashMode：int类型，非必填项，表示闪光灯的模式。\",\n",
      "        \"generation_info\": null,\n",
      "        \"message\": {\n",
      "          \"content\": \"aiRecognize有以下参数：\\n\\n- inputCardMode：int类型，必填项，表示输入卡片的模式。\\n- retryCount：int类型，非必填项，表示识别失败后的重试次数。\\n- maxScanTime：int类型，非必填项，表示最长的扫描时间。\\n- flashMode：int类型，非必填项，表示闪光灯的模式。\",\n",
      "          \"additional_kwargs\": {},\n",
      "          \"example\": false\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"prompt_tokens\": 272,\n",
      "      \"completion_tokens\": 98,\n",
      "      \"total_tokens\": 370\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 2:chain:StuffDocumentsChain > 3:chain:LLMChain] [18.09s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"aiRecognize有以下参数：\\n\\n- inputCardMode：int类型，必填项，表示输入卡片的模式。\\n- retryCount：int类型，非必填项，表示识别失败后的重试次数。\\n- maxScanTime：int类型，非必填项，表示最长的扫描时间。\\n- flashMode：int类型，非必填项，表示闪光灯的模式。\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 2:chain:StuffDocumentsChain] [18.09s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output_text\": \"aiRecognize有以下参数：\\n\\n- inputCardMode：int类型，必填项，表示输入卡片的模式。\\n- retryCount：int类型，非必填项，表示识别失败后的重试次数。\\n- maxScanTime：int类型，非必填项，表示最长的扫描时间。\\n- flashMode：int类型，非必填项，表示闪光灯的模式。\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain] [18.68s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"answer\": \"aiRecognize有以下参数：\\n\\n- inputCardMode：int类型，必填项，表示输入卡片的模式。\\n- retryCount：int类型，非必填项，表示识别失败后的重试次数。\\n- maxScanTime：int类型，非必填项，表示最长的扫描时间。\\n- flashMode：int类型，非必填项，表示闪光灯的模式。\"\n",
      "}\n",
      "aiRecognize有以下参数：\n",
      "\n",
      "- inputCardMode：int类型，必填项，表示输入卡片的模式。\n",
      "- retryCount：int类型，非必填项，表示识别失败后的重试次数。\n",
      "- maxScanTime：int类型，非必填项，表示最长的扫描时间。\n",
      "- flashMode：int类型，非必填项，表示闪光灯的模式。\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "import time\n",
    " \n",
    "_template = \"\"\"Given the following conversation and a follow up question, answer the follow up question in the initial language of the question. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\"\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)\n",
    "\n",
    "prompt_template = \"\"\"Use the following pieces of context to answer the question in the language of the question. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "langchain.debug = True\n",
    "persist_directory = './db/chroma_500/api_doc'\n",
    "db = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n",
    "\n",
    "#----test-start\n",
    "# query = \"aiRecognize有哪些参数\"\n",
    "# docs = db.similarity_search(query)\n",
    "# print(len(docs))\n",
    "# print(docs)\n",
    "#----test-end\n",
    "\n",
    "\n",
    "retriever = db.as_retriever(search_type=\"mmr\")\n",
    "openai_api_key = OPENAI_API_KEY\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\", return_messages=True)\n",
    "examples = ['aiRecognize有哪些参数?', 'aiRecognize需要哪些权限', '请提供aiRecognize的示例代码?']\n",
    "examples = ['aiRecognize有哪些参数?']\n",
    "ConversationalRetrievalChain.prompts = QA_PROMPT\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    ChatOpenAI(\n",
    "        model_name=\"gpt-3.5-turbo\", openai_api_key=openai_api_key, \n",
    "        temperature=\"0.0\", max_tokens=500), \n",
    "        retriever, memory=memory, verbose=True, \n",
    "        max_tokens_limit=1024)\n",
    "for doc in examples:    \n",
    "    query = doc\n",
    "    model_response = qa({\"question\": doc})    \n",
    "    print(model_response['answer'])\n",
    "    #time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5ee1d595-fb0c-40ee-93db-951f71c3bfc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:VectorDBQA] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"query\": \"aiRecognize有哪些参数\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:VectorDBQA > 2:chain:StuffDocumentsChain] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:VectorDBQA > 2:chain:StuffDocumentsChain > 3:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"aiRecognize有哪些参数\",\n",
      "  \"context\": \"kl.aiRecognize\\n\\nAPI名称\\n\\naiRecognize\\n\\n功能描述\\n\\n快驴容器增加视觉识别能力\\n\\n参数说明\\n\\n输入参数\\n\\nschema: 'aiRecognize',\\n})\\n// 调用异步 api\\nif (isSupport) {\\n  myapi.callAsync({\\n    appId: 'value',\\n    businessId: 'value',\\n    isOnline: 'value',\\n    cardType: 'value',\\n    inputCardMode: 'value',\\n\\nretryCount: 'value',\\n    maxScanTime: 'value',\\n    flashMode: 'value',\\n    capacity: 'value',\\n    target: 'value',\\n    strTag: 'value',\\n    maxHeight: 'value',\\n    maxWidth: 'value',\\n    imgPath: 'value',\\n\\n// 创建 api\\nconst myapi = wx.registerExtendedAPI({\\n  supportAsync: True,\\n  supportSync: False,\\n  name: 'aiRecognize',\\n  scope: 'kl',\\n})\\n// 判断当前 App api 是否可用\\nconst isSupport = wx.canIUse({\\n  scope: 'kl',\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:VectorDBQA > 2:chain:StuffDocumentsChain > 3:chain:LLMChain > 4:llm:OpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\nkl.aiRecognize\\n\\nAPI名称\\n\\naiRecognize\\n\\n功能描述\\n\\n快驴容器增加视觉识别能力\\n\\n参数说明\\n\\n输入参数\\n\\nschema: 'aiRecognize',\\n})\\n// 调用异步 api\\nif (isSupport) {\\n  myapi.callAsync({\\n    appId: 'value',\\n    businessId: 'value',\\n    isOnline: 'value',\\n    cardType: 'value',\\n    inputCardMode: 'value',\\n\\nretryCount: 'value',\\n    maxScanTime: 'value',\\n    flashMode: 'value',\\n    capacity: 'value',\\n    target: 'value',\\n    strTag: 'value',\\n    maxHeight: 'value',\\n    maxWidth: 'value',\\n    imgPath: 'value',\\n\\n// 创建 api\\nconst myapi = wx.registerExtendedAPI({\\n  supportAsync: True,\\n  supportSync: False,\\n  name: 'aiRecognize',\\n  scope: 'kl',\\n})\\n// 判断当前 App api 是否可用\\nconst isSupport = wx.canIUse({\\n  scope: 'kl',\\n\\nQuestion: aiRecognize有哪些参数\\nHelpful Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:VectorDBQA > 2:chain:StuffDocumentsChain > 3:chain:LLMChain > 4:llm:OpenAI] [1.76s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \" inputCardMode, retryCount, maxScanTime, flashMode, capacity, target, strTag, maxHeight, maxWidth, imgPath.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 31,\n",
      "      \"prompt_tokens\": 399,\n",
      "      \"total_tokens\": 430\n",
      "    },\n",
      "    \"model_name\": \"text-davinci-003\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:VectorDBQA > 2:chain:StuffDocumentsChain > 3:chain:LLMChain] [1.76s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \" inputCardMode, retryCount, maxScanTime, flashMode, capacity, target, strTag, maxHeight, maxWidth, imgPath.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:VectorDBQA > 2:chain:StuffDocumentsChain] [1.76s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output_text\": \" inputCardMode, retryCount, maxScanTime, flashMode, capacity, target, strTag, maxHeight, maxWidth, imgPath.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:VectorDBQA] [2.38s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"result\": \" inputCardMode, retryCount, maxScanTime, flashMode, capacity, target, strTag, maxHeight, maxWidth, imgPath.\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' inputCardMode, retryCount, maxScanTime, flashMode, capacity, target, strTag, maxHeight, maxWidth, imgPath.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import VectorDBQA\n",
    "\n",
    "qa = VectorDBQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", vectorstore=db)\n",
    "query = \"aiRecognize有哪些参数\"\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fe8784-4508-4699-8b6b-fdfd5ec5240c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c0d390c-64e5-48c7-ac2b-07e6cb53f798",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFileContext(filepath):\n",
    "    # 用'with'关键字自动关闭文件\n",
    "    with open('filename.txt', 'r') as f:\n",
    "        content = f.read()\n",
    "    #print(content)\n",
    "    return content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ad4d28-8237-425d-b9d7-2c5d3eb24e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadFile(filepath):\n",
    "    loader = UnstructuredMarkdownLoader(filepath)\n",
    "    documents = loader.load()\n",
    "    file_sha1 = compute_sha1_from_file(filepath)\n",
    "    file_size = get_file_size(filepath)\n",
    "    return documents, file_sha1, file_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c66b09d6-17c8-4a7b-968f-59db45239d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './api_docs/aiRecognize.md'\n",
    "chunk_size = 500\n",
    "chunk_overlap = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7663e169-4c7e-4ff6-b065-ff73e2931f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation.qa import QAGenerateChain\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "05afa507-66f3-40e4-9617-45987579f776",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "file_documents, file_sha1, file_size = loadFile(file_path)\n",
    "example_gen_chain = QAGenerateChain.from_llm(ChatOpenAI())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3e982f7c-bf93-49d7-b9fd-16d12eec9349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[{'query': 'What is the purpose of the aiRecognize API?', 'answer': 'The aiRecognize API adds visual recognition capabilities to the Quick Donkey container.'}]\n"
     ]
    }
   ],
   "source": [
    "#print(len(file_documents))\n",
    "persist_directory = './db/chroma_800/api_doc'\n",
    "vectordb = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n",
    "#documents = text_splitter.split_documents(file_documents)\n",
    "new_examples = example_gen_chain.apply_and_parse(\n",
    "    [{\"doc\": t} for t in file_documents]\n",
    ")\n",
    "print(new_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c8f0ab66-dee4-41ac-ae0e-584749add70c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RetrievalQA] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"query\": \"What is the purpose of the aiRecognize API?\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 20 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 2:chain:StuffDocumentsChain] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 2:chain:StuffDocumentsChain > 3:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What is the purpose of the aiRecognize API?\",\n",
      "  \"context\": \"kl.aiRecognize\\n\\nAPI名称\\n\\naiRecognize\\n\\n功能描述\\n\\n快驴容器增加视觉识别能力\\n\\n参数说明\\n\\n输入参数\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 2:chain:StuffDocumentsChain > 3:chain:LLMChain > 4:llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: Use the following pieces of context to answer the users question. \\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n----------------\\nkl.aiRecognize\\n\\nAPI名称\\n\\naiRecognize\\n\\n功能描述\\n\\n快驴容器增加视觉识别能力\\n\\n参数说明\\n\\n输入参数\\nHuman: What is the purpose of the aiRecognize API?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 2:chain:StuffDocumentsChain > 3:chain:LLMChain > 4:llm:ChatOpenAI] [1.10s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"The aiRecognize API is used to add visual recognition capabilities to the Fast Donkey container.\",\n",
      "        \"generation_info\": null,\n",
      "        \"message\": {\n",
      "          \"content\": \"The aiRecognize API is used to add visual recognition capabilities to the Fast Donkey container.\",\n",
      "          \"additional_kwargs\": {},\n",
      "          \"example\": false\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"prompt_tokens\": 101,\n",
      "      \"completion_tokens\": 19,\n",
      "      \"total_tokens\": 120\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 2:chain:StuffDocumentsChain > 3:chain:LLMChain] [1.10s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"The aiRecognize API is used to add visual recognition capabilities to the Fast Donkey container.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 2:chain:StuffDocumentsChain] [1.11s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output_text\": \"The aiRecognize API is used to add visual recognition capabilities to the Fast Donkey container.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RetrievalQA] [1.85s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"result\": \"The aiRecognize API is used to add visual recognition capabilities to the Fast Donkey container.\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The aiRecognize API is used to add visual recognition capabilities to the Fast Donkey container.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatOpenAI(temperature = 0.0)\n",
    "retriever = vectordb.as_retriever(search_type=\"mmr\")\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever, \n",
    "    verbose=True,\n",
    "    chain_type_kwargs = {\n",
    "        \"document_separator\": \"<<<<>>>>>\"\n",
    "    }\n",
    ")\n",
    "qa.run(new_examples[0][\"query\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bcbf1e94-5d0f-4f51-8c33-75ad38e879c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain.debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "360fc84c-6fff-4006-9722-346f8beed920",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in on_chain_start callback: 'name'\n",
      "Number of requested results 20 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "predictions = qa.apply(new_examples)\n",
    "from langchain.evaluation.qa import QAEvalChain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "236b19c9-3394-4adf-96f3-1689499e2c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0)\n",
    "eval_chain = QAEvalChain.from_llm(llm)\n",
    "graded_outputs = eval_chain.evaluate(new_examples, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2e99f2e2-efa2-4fa3-bd76-223f572633a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 0:\n",
      "Question: What is the purpose of the aiRecognize API?\n",
      "Real Answer: The aiRecognize API adds visual recognition capabilities to the Quick Donkey container.\n",
      "Predicted Answer: The aiRecognize API is used to add visual recognition capabilities to the Fast Donkey container.\n",
      "Predicted Grade: CORRECT\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, eg in enumerate(new_examples):\n",
    "    print(f\"Example {i}:\")\n",
    "    print(\"Question: \" + predictions[i]['query'])\n",
    "    print(\"Real Answer: \" + predictions[i]['answer'])\n",
    "    print(\"Predicted Answer: \" + predictions[i]['result'])\n",
    "    print(\"Predicted Grade: \" + graded_outputs[i]['text'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d58722-8877-473e-8e5b-fb13eb42f21d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c68075-460b-4300-928c-2c87cbcdb9e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c886a7-7d8e-41de-86c8-f8d37a92cd1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

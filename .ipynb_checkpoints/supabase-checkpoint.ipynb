{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730b1301-6c84-4a0b-b7b4-db0afac5055b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "020b3011-f9eb-487e-9cec-ea5d95c1c8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://pwkdezhnlgswirlkkqqg.supabase.co\n",
      "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InB3a2RlemhubGdzd2lybGtrcXFnIiwicm9sZSI6ImFub24iLCJpYXQiOjE2ODYxOTMzNTgsImV4cCI6MjAwMTc2OTM1OH0.JMsfCXZ4bjeTHtFlcKG80KG0yP15R5V13twxajEaf4c\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from supabase.client import Client, create_client\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "supabase_url = os.environ.get(\"SUPABASE_URL\")\n",
    "supabase_key = os.environ.get(\"SUPABASE_SERVICE_KEY\")\n",
    "print(supabase_url)\n",
    "print(supabase_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0783a032-1b96-4194-9723-1abac3262fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import SupabaseVectorStore\n",
    "from langchain.document_loaders import TextLoader\n",
    "from supabase import Client, create_client\n",
    "from typing import Any, List\n",
    "from langchain.docstore.document import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "70d3b0fc-0ccc-47dc-b2be-7514654d6828",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "supabase_client = create_client(supabase_url, supabase_key)\n",
    "\n",
    "# vector_store = SupabaseVectorStore(\n",
    "#     supabase_client, embeddings, table_name=\"vectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f23b36b0-8cba-428c-89a9-d5c49e4b3d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSupabaseVectorStore(SupabaseVectorStore):\n",
    "    '''A custom vector store that uses the match_vectors table instead of the vectors table.'''\n",
    "    user_id: str\n",
    "    def __init__(self, client: Client, embedding: OpenAIEmbeddings, table_name: str, user_id: str = \"none\"):\n",
    "        super().__init__(client, embedding, table_name)\n",
    "        self.user_id = user_id\n",
    "    \n",
    "    def similarity_search(\n",
    "        self, \n",
    "        query: str, \n",
    "        user_id: str = \"none\",\n",
    "        table: str = \"match_vectors\", \n",
    "        k: int = 4, \n",
    "        threshold: float = 0.5, \n",
    "        **kwargs: Any\n",
    "    ) -> List[Document]:\n",
    "\n",
    "        vectors = self._embedding.embed_documents([query])\n",
    "\n",
    "        query_embedding = vectors[0]\n",
    "        res = self._client.rpc(\n",
    "            table,\n",
    "            {\n",
    "                \"query_embedding\": query_embedding,\n",
    "                \"match_count\": k,\n",
    "                \"p_user_id\": self.user_id,\n",
    "            },\n",
    "        ).execute()\n",
    "\n",
    "        match_result = [\n",
    "            (\n",
    "                Document(\n",
    "                    metadata=search.get(\"metadata\", {}),  # type: ignore\n",
    "                    page_content=search.get(\"content\", \"\"),\n",
    "                ),\n",
    "                search.get(\"similarity\", 0.0),\n",
    "            )\n",
    "            for search in res.data\n",
    "            if search.get(\"content\")\n",
    "        ]\n",
    "\n",
    "        documents = [doc for doc, _ in match_result]\n",
    "\n",
    "\n",
    "        return documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4c9e1ba8-8be4-45e5-9f6d-691ab59294a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.chat_models import ChatOpenAI, ChatVertexAI\n",
    "from langchain.chat_models.anthropic import ChatAnthropic\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.llms import VertexAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.vectorstores import SupabaseVectorStore\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "import LANGUAGE_PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "ceb1c98e-59d3-44d5-b407-32e6b090b5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import UnstructuredMarkdownLoader\n",
    "import hashlib\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "8afb4773-73dd-44d0-a105-1b888a183490",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = \"442106770@qq.com\"\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "documents_vector_store = SupabaseVectorStore(supabase_client, embeddings, table_name=\"vectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "90423b7e-0dfc-4bb7-9f66-02779b13fba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_bytes(bytes, precision=2):\n",
    "    \"\"\"Converts bytes into a human-friendly format.\"\"\"\n",
    "    abbreviations = ['B', 'KB', 'MB']\n",
    "    if bytes <= 0:\n",
    "        return '0 B'\n",
    "    size = bytes\n",
    "    index = 0\n",
    "    while size >= 1024 and index < len(abbreviations) - 1:\n",
    "        size /= 1024\n",
    "        index += 1\n",
    "    return f'{size:.{precision}f} {abbreviations[index]}'\n",
    "\n",
    "def get_file_size(filepath):\n",
    "    file_size = os.path.getsize(filepath)\n",
    "    return file_size\n",
    "\n",
    "def compute_sha1_from_file(file_path):\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        bytes = file.read()\n",
    "        readable_hash = compute_sha1_from_content(bytes)\n",
    "    return readable_hash\n",
    "\n",
    "\n",
    "def compute_sha1_from_content(content):\n",
    "    readable_hash = hashlib.sha1(content).hexdigest()\n",
    "    return readable_hash   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "c75aad90-959a-43a0-871e-75fd1f9f74d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector(user_id,doc):\n",
    "    try:\n",
    "        sids = documents_vector_store.add_documents(\n",
    "            [doc])\n",
    "        if sids and len(sids) > 0:\n",
    "            supabase_client.table(\"vectors\").update(\n",
    "                {\"user_id\": user_id}).match({\"id\": sids[0]}).execute()\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating vector for document {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "8c97ea92-2adb-42b6-b581-85cf4169a62d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'langchain' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[136], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mlangchain\u001b[49m\u001b[38;5;241m.\u001b[39mdebug \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      2\u001b[0m chunk_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m\n\u001b[1;32m      3\u001b[0m chunk_overlap \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'langchain' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "chunk_size = 500\n",
    "chunk_overlap = 0\n",
    "documents = []\n",
    "file_path = './api_docs/aiRecognize.md'\n",
    "loader = UnstructuredMarkdownLoader(file_path)\n",
    "documents = loader.load()\n",
    "print(len(documents))\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "\n",
    "\n",
    "documents = text_splitter.split_documents(documents)\n",
    "print(len(documents))\n",
    "\n",
    "file_sha1 = compute_sha1_from_file(file_path)\n",
    "file_size = get_file_size(file_path)\n",
    "file_name = os.path.basename(file_path)\n",
    "dateshort = time.strftime(\"%Y%m%d\")\n",
    "enable_summarization = False\n",
    "\n",
    "for doc in documents:\n",
    "    metadata = {\n",
    "        \"file_sha1\": file_sha1,\n",
    "        \"file_size\": file_size,\n",
    "        \"file_name\": file_name,\n",
    "        \"chunk_size\": chunk_size,\n",
    "        \"chunk_overlap\": chunk_overlap,\n",
    "        \"date\": dateshort,\n",
    "        \"summarization\": \"true\" if enable_summarization else \"false\"\n",
    "    }\n",
    "    doc_with_metadata = Document(\n",
    "        page_content=doc.page_content, metadata=metadata)\n",
    "    create_vector(user_id, doc_with_metadata)\n",
    "        #     add_usage(stats_db, \"embedding\", \"audio\", metadata={\"file_name\": file_meta_name,\"file_type\": \".txt\", \"chunk_size\": chunk_size, \"chunk_overlap\": chunk_overlap})\n",
    "\n",
    "    # if enable_summarization and ids and len(ids) > 0:\n",
    "    #     create_summary(ids[0], doc.page_content, metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "5330fbd7-2423-41be-99c0-81e683b47ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import SupabaseVectorStore\n",
    "import langchain\n",
    "\n",
    "langchain.debug = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "37096f99-8472-4db5-be21-7a1dc00a5dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 2:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"aiRecognize使用示例代码\",\n",
      "  \"chat_history\": \"\\nHuman: aiRecognizeAPI示例代码\\nAssistant: 很抱歉，我无法提供aiRecognizeAPI的示例代码，因为上下文中没有提供。\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 2:chain:LLMChain > 3:llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n\\nHuman: aiRecognizeAPI示例代码\\nAssistant: 很抱歉，我无法提供aiRecognizeAPI的示例代码，因为上下文中没有提供。\\nFollow Up Input: aiRecognize使用示例代码\\nStandalone question:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 2:chain:LLMChain > 3:llm:ChatOpenAI] [1.44s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"请提供aiRecognizeAPI的使用示例代码。\",\n",
      "        \"generation_info\": null,\n",
      "        \"message\": {\n",
      "          \"content\": \"请提供aiRecognizeAPI的使用示例代码。\",\n",
      "          \"additional_kwargs\": {},\n",
      "          \"example\": false\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"prompt_tokens\": 97,\n",
      "      \"completion_tokens\": 13,\n",
      "      \"total_tokens\": 110\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 2:chain:LLMChain] [1.44s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"请提供aiRecognizeAPI的使用示例代码。\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 4:chain:StuffDocumentsChain] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 4:chain:StuffDocumentsChain > 5:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"请提供aiRecognizeAPI的使用示例代码。\",\n",
      "  \"context\": \"kl.aiRecognize\\n\\nAPI名称\\n\\naiRecognize\\n\\n功能描述\\n\\n快驴容器增加视觉识别能力\\n\\n参数说明\\n\\n输入参数\\n\\njavascript\\n// 仅供业务同学参考，具体代码需要基于业务实际场景实现\\nif (typeof mmp === 'undefined' || typeof mmp.registerExtendedEvent !== 'function') {\\n  return\\n}\\n// 创建 api\\nconst myapi = wx.registerExtendedAPI({\\n  supportAsync: True,\\n  supportSync: False,\\n  name: 'aiRecognize',\\n  scope: 'kl',\\n})\\n// 判断当前 App api 是否可用\\nconst isSupport = wx.canIUse({\\n  scope: 'kl',\\n  schema: 'aiRecognize',\\n})\\n// 调用异步 api\\nif (isSupport) {\\n  myapi.callAsync({\\n    appId: 'value',\\n    businessId: 'value',\\n    isOnline: 'value',\\n    cardType: 'value',\\n    inputCardMode: 'value',\\n    retryCount: 'value',\\n    maxScanTime: 'value',\\n    flashMode: 'value',\\n    capacity: 'value',\\n    target: 'value',\\n    strTag: 'value',\\n    maxHeight: 'value',\\n    maxWidth: 'value',\\n    imgPath: 'value',\\n    strReserve: 'value',\\n    _mt: 'value',\\n    success(res) {\\n      console.log('success', res)\\n    },\\n    fail(err) {\\n      console.log('fail', err)\\n    }\\n  })\\n} else {\\n\\n| 属性 | 类型 | 说明 |\\n| --- | --- | --- |\\n| imgPath | string | imgPath |\\n| recResult | string | recResult |\\n| type | int | type |\\n| code | int | code |\\n| message | string | message |\\n\\n示例代码\\n\\n隐私信息\\n\\nAndroid需要隐私权限: PermissionGuard.PERMISSION_CAMERA,PermissionGuard.PERMISSION_STORAGE_READ\\niOS需要隐私权限: PMPermissionIDCamera\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 4:chain:StuffDocumentsChain > 5:chain:LLMChain > 6:llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: Use the following pieces of context to answer the users question. \\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n----------------\\nkl.aiRecognize\\n\\nAPI名称\\n\\naiRecognize\\n\\n功能描述\\n\\n快驴容器增加视觉识别能力\\n\\n参数说明\\n\\n输入参数\\n\\njavascript\\n// 仅供业务同学参考，具体代码需要基于业务实际场景实现\\nif (typeof mmp === 'undefined' || typeof mmp.registerExtendedEvent !== 'function') {\\n  return\\n}\\n// 创建 api\\nconst myapi = wx.registerExtendedAPI({\\n  supportAsync: True,\\n  supportSync: False,\\n  name: 'aiRecognize',\\n  scope: 'kl',\\n})\\n// 判断当前 App api 是否可用\\nconst isSupport = wx.canIUse({\\n  scope: 'kl',\\n  schema: 'aiRecognize',\\n})\\n// 调用异步 api\\nif (isSupport) {\\n  myapi.callAsync({\\n    appId: 'value',\\n    businessId: 'value',\\n    isOnline: 'value',\\n    cardType: 'value',\\n    inputCardMode: 'value',\\n    retryCount: 'value',\\n    maxScanTime: 'value',\\n    flashMode: 'value',\\n    capacity: 'value',\\n    target: 'value',\\n    strTag: 'value',\\n    maxHeight: 'value',\\n    maxWidth: 'value',\\n    imgPath: 'value',\\n    strReserve: 'value',\\n    _mt: 'value',\\n    success(res) {\\n      console.log('success', res)\\n    },\\n    fail(err) {\\n      console.log('fail', err)\\n    }\\n  })\\n} else {\\n\\n| 属性 | 类型 | 说明 |\\n| --- | --- | --- |\\n| imgPath | string | imgPath |\\n| recResult | string | recResult |\\n| type | int | type |\\n| code | int | code |\\n| message | string | message |\\n\\n示例代码\\n\\n隐私信息\\n\\nAndroid需要隐私权限: PermissionGuard.PERMISSION_CAMERA,PermissionGuard.PERMISSION_STORAGE_READ\\niOS需要隐私权限: PMPermissionIDCamera\\nHuman: 请提供aiRecognizeAPI的使用示例代码。\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 4:chain:StuffDocumentsChain > 5:chain:LLMChain > 6:llm:ChatOpenAI] [15.66s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"由于该API的具体实现需要基于业务实际场景实现，因此无法提供完整的使用示例代码。但是，以下是一个基本的示例代码，供您参考：\\n\\n```javascript\\n// 创建 api\\nconst myapi = wx.registerExtendedAPI({\\n  supportAsync: true,\\n  supportSync: false,\\n  name: 'aiRecognize',\\n  scope: 'kl',\\n})\\n\\n// 判断当前 App api 是否可用\\nconst isSupport = wx.canIUse({\\n  scope: 'kl',\\n  schema: 'aiRecognize',\\n})\\n\\n// 调用异步 api\\nif (isSupport) {\\n  myapi.callAsync({\\n    appId: 'value',\\n    businessId: 'value',\\n    isOnline: 'value',\\n    cardType: 'value',\\n    inputCardMode: 'value',\\n    retryCount: 'value',\\n    maxScanTime: 'value',\\n    flashMode: 'value',\\n    capacity: 'value',\\n    target: 'value',\\n    strTag: 'value',\\n    maxHeight: 'value',\\n    maxWidth: 'value',\\n    imgPath: 'value',\\n    strReserve: 'value',\\n    _mt: 'value',\\n    success(res) {\\n      console.log('success', res)\\n    },\\n    fail(err) {\\n      console.log('fail', err)\\n    }\\n  })\\n} else {\\n  console.log('当前 App 不支持 aiRecognize API')\\n}\\n```\\n\\n请注意，上述代码仅供参考，具体实现需要根据业务实际场景进行调整。\",\n",
      "        \"generation_info\": null,\n",
      "        \"message\": {\n",
      "          \"content\": \"由于该API的具体实现需要基于业务实际场景实现，因此无法提供完整的使用示例代码。但是，以下是一个基本的示例代码，供您参考：\\n\\n```javascript\\n// 创建 api\\nconst myapi = wx.registerExtendedAPI({\\n  supportAsync: true,\\n  supportSync: false,\\n  name: 'aiRecognize',\\n  scope: 'kl',\\n})\\n\\n// 判断当前 App api 是否可用\\nconst isSupport = wx.canIUse({\\n  scope: 'kl',\\n  schema: 'aiRecognize',\\n})\\n\\n// 调用异步 api\\nif (isSupport) {\\n  myapi.callAsync({\\n    appId: 'value',\\n    businessId: 'value',\\n    isOnline: 'value',\\n    cardType: 'value',\\n    inputCardMode: 'value',\\n    retryCount: 'value',\\n    maxScanTime: 'value',\\n    flashMode: 'value',\\n    capacity: 'value',\\n    target: 'value',\\n    strTag: 'value',\\n    maxHeight: 'value',\\n    maxWidth: 'value',\\n    imgPath: 'value',\\n    strReserve: 'value',\\n    _mt: 'value',\\n    success(res) {\\n      console.log('success', res)\\n    },\\n    fail(err) {\\n      console.log('fail', err)\\n    }\\n  })\\n} else {\\n  console.log('当前 App 不支持 aiRecognize API')\\n}\\n```\\n\\n请注意，上述代码仅供参考，具体实现需要根据业务实际场景进行调整。\",\n",
      "          \"additional_kwargs\": {},\n",
      "          \"example\": false\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"prompt_tokens\": 493,\n",
      "      \"completion_tokens\": 345,\n",
      "      \"total_tokens\": 838\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 4:chain:StuffDocumentsChain > 5:chain:LLMChain] [15.66s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"由于该API的具体实现需要基于业务实际场景实现，因此无法提供完整的使用示例代码。但是，以下是一个基本的示例代码，供您参考：\\n\\n```javascript\\n// 创建 api\\nconst myapi = wx.registerExtendedAPI({\\n  supportAsync: true,\\n  supportSync: false,\\n  name: 'aiRecognize',\\n  scope: 'kl',\\n})\\n\\n// 判断当前 App api 是否可用\\nconst isSupport = wx.canIUse({\\n  scope: 'kl',\\n  schema: 'aiRecognize',\\n})\\n\\n// 调用异步 api\\nif (isSupport) {\\n  myapi.callAsync({\\n    appId: 'value',\\n    businessId: 'value',\\n    isOnline: 'value',\\n    cardType: 'value',\\n    inputCardMode: 'value',\\n    retryCount: 'value',\\n    maxScanTime: 'value',\\n    flashMode: 'value',\\n    capacity: 'value',\\n    target: 'value',\\n    strTag: 'value',\\n    maxHeight: 'value',\\n    maxWidth: 'value',\\n    imgPath: 'value',\\n    strReserve: 'value',\\n    _mt: 'value',\\n    success(res) {\\n      console.log('success', res)\\n    },\\n    fail(err) {\\n      console.log('fail', err)\\n    }\\n  })\\n} else {\\n  console.log('当前 App 不支持 aiRecognize API')\\n}\\n```\\n\\n请注意，上述代码仅供参考，具体实现需要根据业务实际场景进行调整。\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 4:chain:StuffDocumentsChain] [15.66s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output_text\": \"由于该API的具体实现需要基于业务实际场景实现，因此无法提供完整的使用示例代码。但是，以下是一个基本的示例代码，供您参考：\\n\\n```javascript\\n// 创建 api\\nconst myapi = wx.registerExtendedAPI({\\n  supportAsync: true,\\n  supportSync: false,\\n  name: 'aiRecognize',\\n  scope: 'kl',\\n})\\n\\n// 判断当前 App api 是否可用\\nconst isSupport = wx.canIUse({\\n  scope: 'kl',\\n  schema: 'aiRecognize',\\n})\\n\\n// 调用异步 api\\nif (isSupport) {\\n  myapi.callAsync({\\n    appId: 'value',\\n    businessId: 'value',\\n    isOnline: 'value',\\n    cardType: 'value',\\n    inputCardMode: 'value',\\n    retryCount: 'value',\\n    maxScanTime: 'value',\\n    flashMode: 'value',\\n    capacity: 'value',\\n    target: 'value',\\n    strTag: 'value',\\n    maxHeight: 'value',\\n    maxWidth: 'value',\\n    imgPath: 'value',\\n    strReserve: 'value',\\n    _mt: 'value',\\n    success(res) {\\n      console.log('success', res)\\n    },\\n    fail(err) {\\n      console.log('fail', err)\\n    }\\n  })\\n} else {\\n  console.log('当前 App 不支持 aiRecognize API')\\n}\\n```\\n\\n请注意，上述代码仅供参考，具体实现需要根据业务实际场景进行调整。\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain] [18.63s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"answer\": \"由于该API的具体实现需要基于业务实际场景实现，因此无法提供完整的使用示例代码。但是，以下是一个基本的示例代码，供您参考：\\n\\n```javascript\\n// 创建 api\\nconst myapi = wx.registerExtendedAPI({\\n  supportAsync: true,\\n  supportSync: false,\\n  name: 'aiRecognize',\\n  scope: 'kl',\\n})\\n\\n// 判断当前 App api 是否可用\\nconst isSupport = wx.canIUse({\\n  scope: 'kl',\\n  schema: 'aiRecognize',\\n})\\n\\n// 调用异步 api\\nif (isSupport) {\\n  myapi.callAsync({\\n    appId: 'value',\\n    businessId: 'value',\\n    isOnline: 'value',\\n    cardType: 'value',\\n    inputCardMode: 'value',\\n    retryCount: 'value',\\n    maxScanTime: 'value',\\n    flashMode: 'value',\\n    capacity: 'value',\\n    target: 'value',\\n    strTag: 'value',\\n    maxHeight: 'value',\\n    maxWidth: 'value',\\n    imgPath: 'value',\\n    strReserve: 'value',\\n    _mt: 'value',\\n    success(res) {\\n      console.log('success', res)\\n    },\\n    fail(err) {\\n      console.log('fail', err)\\n    }\\n  })\\n} else {\\n  console.log('当前 App 不支持 aiRecognize API')\\n}\\n```\\n\\n请注意，上述代码仅供参考，具体实现需要根据业务实际场景进行调整。\"\n",
      "}\n",
      "由于该API的具体实现需要基于业务实际场景实现，因此无法提供完整的使用示例代码。但是，以下是一个基本的示例代码，供您参考：\n",
      "\n",
      "```javascript\n",
      "// 创建 api\n",
      "const myapi = wx.registerExtendedAPI({\n",
      "  supportAsync: true,\n",
      "  supportSync: false,\n",
      "  name: 'aiRecognize',\n",
      "  scope: 'kl',\n",
      "})\n",
      "\n",
      "// 判断当前 App api 是否可用\n",
      "const isSupport = wx.canIUse({\n",
      "  scope: 'kl',\n",
      "  schema: 'aiRecognize',\n",
      "})\n",
      "\n",
      "// 调用异步 api\n",
      "if (isSupport) {\n",
      "  myapi.callAsync({\n",
      "    appId: 'value',\n",
      "    businessId: 'value',\n",
      "    isOnline: 'value',\n",
      "    cardType: 'value',\n",
      "    inputCardMode: 'value',\n",
      "    retryCount: 'value',\n",
      "    maxScanTime: 'value',\n",
      "    flashMode: 'value',\n",
      "    capacity: 'value',\n",
      "    target: 'value',\n",
      "    strTag: 'value',\n",
      "    maxHeight: 'value',\n",
      "    maxWidth: 'value',\n",
      "    imgPath: 'value',\n",
      "    strReserve: 'value',\n",
      "    _mt: 'value',\n",
      "    success(res) {\n",
      "      console.log('success', res)\n",
      "    },\n",
      "    fail(err) {\n",
      "      console.log('fail', err)\n",
      "    }\n",
      "  })\n",
      "} else {\n",
      "  console.log('当前 App 不支持 aiRecognize API')\n",
      "}\n",
      "```\n",
      "\n",
      "请注意，上述代码仅供参考，具体实现需要根据业务实际场景进行调整。\n"
     ]
    }
   ],
   "source": [
    "vector_store = CustomSupabaseVectorStore(\n",
    "    supabase_client, embeddings, table_name=\"vectors\", user_id=user_id)\n",
    "\n",
    "openai_api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "ConversationalRetrievalChain.prompts = LANGUAGE_PROMPT\n",
    "qa = None\n",
    "# this overwrites the built-in prompt of the ConversationalRetrievalChain\n",
    "ConversationalRetrievalChain.prompts = LANGUAGE_PROMPT\n",
    "query = \"aiRecognize有哪些参数\"\n",
    "\n",
    "query = \"我想实现视觉相关功能有哪些API可以支持\"\n",
    "query = \"aiRecognize使用示例代码\"\n",
    "#query = \"aiRecognize示例代码\"\n",
    "# qa = ConversationalRetrievalChain.from_llm(\n",
    "#     ChatOpenAI(\n",
    "#         model_name=\"gpt-3.5-turbo\", openai_api_key=openai_api_key, \n",
    "#         temperature=\"0.0\", max_tokens=1024), \n",
    "#         vector_store.as_retriever(), memory=memory, verbose=True, \n",
    "#         max_tokens_limit=1024)\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    ChatOpenAI(\n",
    "        model_name=\"gpt-3.5-turbo\", openai_api_key=openai_api_key, \n",
    "        temperature=\"0.0\", max_tokens=1024), \n",
    "        vector_store.as_retriever(), verbose=True, \n",
    "        max_tokens_limit=1024)\n",
    "model_response = qa({\"question\": query})\n",
    "print(model_response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7a1f5210-fbdb-4ea5-991e-34d32410d661",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# matched_docs = vector_store.similarity_search(query)\n",
    "# print(matched_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6f9ffa-fd85-4a5d-83bf-cd8ac536a170",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e54f3a5-872c-4d8b-9a6b-fb468e990043",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_bytes(bytes, precision=2):\n",
    "    \"\"\"Converts bytes into a human-friendly format.\"\"\"\n",
    "    abbreviations = ['B', 'KB', 'MB']\n",
    "    if bytes <= 0:\n",
    "        return '0 B'\n",
    "    size = bytes\n",
    "    index = 0\n",
    "    while size >= 1024 and index < len(abbreviations) - 1:\n",
    "        size /= 1024\n",
    "        index += 1\n",
    "    return f'{size:.{precision}f} {abbreviations[index]}'\n",
    "\n",
    "def get_file_size(filepath):\n",
    "    file_size = os.path.getsize(filepath)\n",
    "    return file_size\n",
    "\n",
    "def compute_sha1_from_file(file_path):\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        bytes = file.read()\n",
    "        readable_hash = compute_sha1_from_content(bytes)\n",
    "    return readable_hash\n",
    "\n",
    "\n",
    "def compute_sha1_from_content(content):\n",
    "    readable_hash = hashlib.sha1(content).hexdigest()\n",
    "    return readable_hash  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e769d73d-a62a-49a8-b12e-fcdabef4d8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadFile(filepath):\n",
    "    loader = UnstructuredMarkdownLoader(filepath)\n",
    "    documents = loader.load()\n",
    "    file_sha1 = compute_sha1_from_file(filepath)\n",
    "    file_size = get_file_size(filepath)\n",
    "    return documents, file_sha1, file_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08be7307-6a6e-4b04-bfa2-3bedc5057708",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 500\n",
    "chunk_overlap = 0\n",
    "file_path = './api_docs/aiRecognize.md'\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "file_documents, file_sha1, file_size = loadFile(file_path)\n",
    "documents = text_splitter.split_documents(file_documents)\n",
    "file_name = os.path.basename(file_path)\n",
    "dateshort = time.strftime(\"%Y%m%d\")\n",
    "enable_summarization = False\n",
    "print(len(documents))\n",
    "vector_store = SupabaseVectorStore.from_documents(\n",
    "    docs, embeddings, client=supabase\n",
    ")\n",
    "for doc in documents:\n",
    "    metadata = {\n",
    "        \"file_sha1\": file_sha1,\n",
    "        \"file_size\": file_size,\n",
    "        \"file_name\": file_name,\n",
    "        \"chunk_size\": chunk_size,\n",
    "        \"chunk_overlap\": chunk_overlap,\n",
    "        \"date\": dateshort,\n",
    "        \"summarization\": \"true\" if enable_summarization else \"false\"\n",
    "    }\n",
    "    doc_with_metadata = Document(page_content=doc.page_content, metadata=metadata)\n",
    "    vector_store = SupabaseVectorStore.from_documents(\n",
    "        [doc_with_metadata], embeddings, client=supabase\n",
    "    )    \n",
    "    print(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f8c432-8bce-42b0-966d-05819bef2df7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3551b576-7e53-44eb-8df2-4d56d994decd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24561a3-bbc5-4601-ad11-346933a08d25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e89121-f8e8-493c-af9c-b0363bf200ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9450bb2-1ff6-47de-b463-e476485c99af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23f238a-8fe2-4c3c-922d-e2ad8baac081",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33b7928-6c39-4d41-b875-d462d149cb24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c643d1f6-3f7f-41a9-ba4d-9935b2be5476",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1682f48-a2bb-41e3-8dca-0c72c3a79ae3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eae41ab-e188-43e9-8afb-6127ac6ba7d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9838e697-7096-462c-9f03-6c7db3fb7748",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec5e333-8259-4386-a662-edc002e2ef28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
